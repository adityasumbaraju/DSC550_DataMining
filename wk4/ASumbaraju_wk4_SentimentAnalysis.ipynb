{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import unicodedata\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day of Week</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monday</td>\n",
       "      <td>Hello, how are you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Today is a good day!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wednesday</td>\n",
       "      <td>It's my birthday so it's a really special day!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thursday</td>\n",
       "      <td>Today is neither a good day or a bad day!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Friday</td>\n",
       "      <td>I'm having a bad day.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Saturday</td>\n",
       "      <td>There' s nothing special happening today.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sunday</td>\n",
       "      <td>Today is a SUPER good day!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Day of Week                                        comments\n",
       "0      Monday                             Hello, how are you?\n",
       "1     Tuesday                            Today is a good day!\n",
       "2   Wednesday  It's my birthday so it's a really special day!\n",
       "3    Thursday       Today is neither a good day or a bad day!\n",
       "4      Friday                           I'm having a bad day.\n",
       "5    Saturday       There' s nothing special happening today.\n",
       "6      Sunday                      Today is a SUPER good day!"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Importing the DailyComments file into a data frame\n",
    "df = pd.read_csv('./datafiles/DailyComments.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are', 'bad', 'birthday', 'day', 'good', 'happening', 'having', 'hello', 'how', 'is', 'it', 'my', 'neither', 'nothing', 'or', 'really', 'so', 'special', 'super', 'there', 'today', 'you']\n"
     ]
    }
   ],
   "source": [
    "# Identify a scheme to categorize each comment as positive or negative\n",
    "\n",
    "#uses count vectorizer transformer (page 58)\n",
    "corpus = df['comments']\n",
    "vectorizer = CountVectorizer()\n",
    "vectors = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Look at vectorized words, I'm using this to determine my positive and negative words (pg 113)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              Hello how are you\n",
       "1                            Today is a good day\n",
       "2    Its my birthday so its a really special day\n",
       "3       Today is neither a good day or a bad day\n",
       "4                            Im having a bad day\n",
       "5        There s nothing special happening today\n",
       "6                      Today is a SUPER good day\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing all punctuation from the text\n",
    "\n",
    "punctuation = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "df['text'] = [string.translate(punctuation) for string in df['comments']]\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                               [Hello, how, are, you]\n",
       "1                            [Today, is, a, good, day]\n",
       "2    [Its, my, birthday, so, its, a, really, specia...\n",
       "3    [Today, is, neither, a, good, day, or, a, bad,...\n",
       "4                            [Im, having, a, bad, day]\n",
       "5       [There, s, nothing, special, happening, today]\n",
       "6                     [Today, is, a, SUPER, good, day]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenizing the words and appending each list of tokenized words into a list to be added to the dataframe\n",
    "tokenized_list = []\n",
    "for string in df['text']:\n",
    "    tokenized = word_tokenize(string)\n",
    "    tokenized_list.append(tokenized)\n",
    "\n",
    "df['text'] = tokenized_list\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Hello', 'NNP'), ('how', 'WRB'), ('are', 'VBP'), ('you', 'PRP')],\n",
       " [('Today', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('good', 'JJ'), ('day', 'NN')],\n",
       " [('Its', 'PRP$'),\n",
       "  ('my', 'PRP$'),\n",
       "  ('birthday', 'NN'),\n",
       "  ('so', 'IN'),\n",
       "  ('its', 'PRP$'),\n",
       "  ('a', 'DT'),\n",
       "  ('really', 'RB'),\n",
       "  ('special', 'JJ'),\n",
       "  ('day', 'NN')],\n",
       " [('Today', 'NN'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('neither', 'CC'),\n",
       "  ('a', 'DT'),\n",
       "  ('good', 'JJ'),\n",
       "  ('day', 'NN'),\n",
       "  ('or', 'CC'),\n",
       "  ('a', 'DT'),\n",
       "  ('bad', 'JJ'),\n",
       "  ('day', 'NN')],\n",
       " [('Im', 'NNP'), ('having', 'VBG'), ('a', 'DT'), ('bad', 'JJ'), ('day', 'NN')],\n",
       " [('There', 'EX'),\n",
       "  ('s', 'VBZ'),\n",
       "  ('nothing', 'NN'),\n",
       "  ('special', 'JJ'),\n",
       "  ('happening', 'VBG'),\n",
       "  ('today', 'NN')],\n",
       " [('Today', 'NN'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('a', 'DT'),\n",
       "  ('SUPER', 'JJ'),\n",
       "  ('good', 'JJ'),\n",
       "  ('day', 'NN')]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying POS tagging to each comment\n",
    "comments_tagged = [pos_tag(comment) for comment in df['text']]\n",
    "comments_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Hello', 'n'), ('how', None), ('are', 'v'), ('you', None)],\n",
       " [('Today', 'n'), ('is', 'v'), ('a', None), ('good', 'a'), ('day', 'n')],\n",
       " [('Its', None),\n",
       "  ('my', None),\n",
       "  ('birthday', 'n'),\n",
       "  ('so', None),\n",
       "  ('its', None),\n",
       "  ('a', None),\n",
       "  ('really', 'r'),\n",
       "  ('special', 'a'),\n",
       "  ('day', 'n')],\n",
       " [('Today', 'n'),\n",
       "  ('is', 'v'),\n",
       "  ('neither', None),\n",
       "  ('a', None),\n",
       "  ('good', 'a'),\n",
       "  ('day', 'n'),\n",
       "  ('or', None),\n",
       "  ('a', None),\n",
       "  ('bad', 'a'),\n",
       "  ('day', 'n')],\n",
       " [('Im', 'n'), ('having', 'v'), ('a', None), ('bad', 'a'), ('day', 'n')],\n",
       " [('There', None),\n",
       "  ('s', 'v'),\n",
       "  ('nothing', 'n'),\n",
       "  ('special', 'a'),\n",
       "  ('happening', 'v'),\n",
       "  ('today', 'n')],\n",
       " [('Today', 'n'),\n",
       "  ('is', 'v'),\n",
       "  ('a', None),\n",
       "  ('SUPER', 'a'),\n",
       "  ('good', 'a'),\n",
       "  ('day', 'n')]]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying text lemmatization using a series of functions\n",
    "\n",
    "#Converting tags from a POS tag vector into letters to run through the lemmatizer function\n",
    "def pos_tagger(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "#Function that takes a list of POS tag vectors and switches the POS tags for \n",
    "#letters that can be input into the lemmatizer function\n",
    "def POS_simplifier(pos_tag_list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    wordnet_tagged = [list(map(lambda x: (x[0], pos_tagger(x[1])), comments_tagged[i])) for i in range(len(pos_tag_list))]            \n",
    "    return wordnet_tagged\n",
    "\n",
    "tagged_comments = POS_simplifier(comments_tagged)\n",
    "tagged_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hello', 'how', 'be', 'you'],\n",
       " ['Today', 'be', 'a', 'good', 'day'],\n",
       " ['Its', 'my', 'birthday', 'so', 'its', 'a', 'really', 'special', 'day'],\n",
       " ['Today', 'be', 'neither', 'a', 'good', 'day', 'or', 'a', 'bad', 'day'],\n",
       " ['Im', 'have', 'a', 'bad', 'day'],\n",
       " ['There', 's', 'nothing', 'special', 'happen', 'today'],\n",
       " ['Today', 'be', 'a', 'SUPER', 'good', 'day']]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_sentences = []\n",
    "for sublist in tagged_comments:\n",
    "    lemmatized_sentences_sub = []\n",
    "    for word, tag in sublist:\n",
    "        if tag is None:\n",
    "            # if there is no available tag, append the token as is\n",
    "            lemmatized_sentences_sub.append(word)\n",
    "        else:        \n",
    "            # else use the tag to lemmatize the token\n",
    "            lemmatized_sentences_sub.append(lemmatizer.lemmatize(word, tag))\n",
    "    lemmatized_sentences.append(lemmatized_sentences_sub)\n",
    "    \n",
    "lemmatized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello how be you',\n",
       " 'Today be a good day',\n",
       " 'Its my birthday so its a really special day',\n",
       " 'Today be neither a good day or a bad day',\n",
       " 'Im have a bad day',\n",
       " 'There s nothing special happen today',\n",
       " 'Today be a SUPER good day']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def listToString(s): \n",
    "    str1 = \" \"   \n",
    "    return str1.join(s)\n",
    "\n",
    "new_list = []\n",
    "for sublist in lemmatized_sentences:\n",
    "    new_list.append(listToString(sublist))\n",
    "\n",
    "\n",
    "new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} Hello how be you\n",
      "{'neg': 0.0, 'neu': 0.58, 'pos': 0.42, 'compound': 0.4404} Today be a good day\n",
      "{'neg': 0.0, 'neu': 0.728, 'pos': 0.272, 'compound': 0.4576} Its my birthday so its a really special day\n",
      "{'neg': 0.425, 'neu': 0.575, 'pos': 0.0, 'compound': -0.7101} Today be neither a good day or a bad day\n",
      "{'neg': 0.467, 'neu': 0.533, 'pos': 0.0, 'compound': -0.5423} Im have a bad day\n",
      "{'neg': 0.311, 'neu': 0.689, 'pos': 0.0, 'compound': -0.3089} There s nothing special happen today\n",
      "{'neg': 0.0, 'neu': 0.347, 'pos': 0.653, 'compound': 0.8192} Today be a SUPER good day\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for comment in new_list:\n",
    "    print(analyzer.polarity_scores(comment),comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#If the compound value is > 0.05 then the sentense is rated as positive and compound value is less than 0.05 then it is rated as negitive sentence.\n",
    "From the above output \n",
    "\n",
    "#sentence 1: compound is neutral (compound value is 0.0)\n",
    "\n",
    "#sentence 2: compound value is positive  (compound: 0.4404)\n",
    "\n",
    "#sentence 3: compound value is positive  (compound: 0.4576)\n",
    "\n",
    "#sentence 4: compound value is negitive  (compound: -0.7101)\n",
    "\n",
    "#sentence 5: compound value is negitive  (compound: -0.5423)\n",
    "\n",
    "#sentence 6: compound value is negitive  (compound: -0.3089)\n",
    "\n",
    "#sentence 7: compound value is positive  (compound: -0.8192)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Score is: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemmatized_sentences</th>\n",
       "      <th>super</th>\n",
       "      <th>good</th>\n",
       "      <th>special</th>\n",
       "      <th>fantastic</th>\n",
       "      <th>bad</th>\n",
       "      <th>totalpositive</th>\n",
       "      <th>totalnegative</th>\n",
       "      <th>ScoreValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello, how are you?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Today is a good day!</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's my birthday so it's a really special day!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Today is neither a good day or a bad day!</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm having a bad day.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>There' s nothing special happening today.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Today is a SUPER good day!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             lemmatized_sentences  super  good  special  \\\n",
       "0                             Hello, how are you?      0     0        0   \n",
       "1                            Today is a good day!      0     1        0   \n",
       "2  It's my birthday so it's a really special day!      0     0        1   \n",
       "3       Today is neither a good day or a bad day!      0     1        0   \n",
       "4                           I'm having a bad day.      0     0        0   \n",
       "5       There' s nothing special happening today.      0     0        1   \n",
       "6                      Today is a SUPER good day!      1     1        0   \n",
       "\n",
       "   fantastic  bad  totalpositive  totalnegative  ScoreValue  \n",
       "0          0    0              0              0           0  \n",
       "1          0    0              1              0           1  \n",
       "2          0    0              1              0           1  \n",
       "3          0    1              1              1           0  \n",
       "4          0    1              0              1          -1  \n",
       "5          0    0              1              0           1  \n",
       "6          0    0              2              0           2  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame({'lemmatized_sentences' : corpus})\n",
    "\n",
    "# check for positive words and negative words\n",
    "# Positive Words\n",
    "df1['super'] = df1.lemmatized_sentences.str.count('SUPER')\n",
    "df1['good'] = df1.lemmatized_sentences.str.count('good')\n",
    "df1['special']= df1.lemmatized_sentences.str.count('special')\n",
    "df1['fantastic']= df1.lemmatized_sentences.str.count('fantastic')\n",
    "\n",
    "\n",
    "# Negative Words\n",
    "df1['bad'] = df1.lemmatized_sentences.str.count('bad')\n",
    "\n",
    "# Totalscores\n",
    "df1['totalpositive'] = (df1.super + df1.good + df1.special + df1.fantastic)\n",
    "df1['totalnegative'] = (df1.bad)\n",
    "\n",
    "#Calculate total score by summing positive and negitive words\n",
    "df1['ScoreValue'] = (df1.totalpositive) - (df1.totalnegative)\n",
    "df1 = pd.DataFrame(df1)\n",
    "\n",
    "\n",
    "Z = sum(df1['ScoreValue'])\n",
    "\n",
    "print(\"Total Score is:\",Z)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For up to 5% extra credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of 0      @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1      is upset that he can't update his Facebook by ...\n",
       "2      @Kenichan I dived many times for the ball. Man...\n",
       "3        my whole body feels itchy and like its on fire \n",
       "4      @nationwideclass no, it's not behaving at all....\n",
       "                             ...                        \n",
       "264                    i have to take my sidekick back. \n",
       "265    @chriscantore congrats! I'm totally jealous! o...\n",
       "266                          gr8t my face is very itchy \n",
       "267    poor socks  luvvvvv the golden retriever!! I w...\n",
       "268    I just saw that they found that Tracy girl in ...\n",
       "Name: Tweet, Length: 269, dtype: object>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. Importing the Dailytweets file into a data frame\n",
    "df_tweet = pd.read_csv('tweets.csv', encoding = \"cp1252\")\n",
    "\n",
    "tweets_c = df_tweet.Tweet.copy()\n",
    "tweets_c.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"is upset that he can't update his Facebook by texting it... and might cry as a result School today also.\""
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to remove special charecters for list.\n",
    "def removeWordWithSC(text, char_list):\n",
    "    #Remove words in a text that contains a special charecters from the list.\n",
    "    text = text.split()\n",
    "    res = [ele for ele in text if all(ch not in ele for ch in char_list)]\n",
    "    res = ' '.join(res)\n",
    "    return res\n",
    "\n",
    "char_list = ['@', '#', 'http', 'www', '/', '!']\n",
    "\n",
    "removeWordWithSC(tweets_c[1], char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"- that's a bummer. You shoulda got David Carr of Third Day to do it. ;D\""
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_cleaned = []\n",
    "for t in tweets_c:\n",
    "    tweets_cleaned.append(removeWordWithSC(t, char_list))\n",
    "    \n",
    "tweets_cleaned[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts):\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    texts_tokens = []\n",
    "    for i, val in enumerate(texts):\n",
    "        text_tokens = tokenizer.tokenize(val.lower())\n",
    "\n",
    "        for i in range(len(text_tokens) - 1, -1, -1):\n",
    "            if len(text_tokens[i]) < 4:\n",
    "                del(text_tokens[i])\n",
    "\n",
    "        texts_tokens.append(text_tokens)\n",
    "        \n",
    "    return texts_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['that', 'bummer', 'shoulda', 'david', 'carr', 'third']]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_tokens = tokenize(tweets_cleaned)\n",
    "\n",
    "tweets_tokens[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSW(texts_tokens):\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    texts_filtered = []\n",
    "\n",
    "    for i, val in enumerate(texts_tokens):\n",
    "        text_filtered = []\n",
    "        for w in val:\n",
    "            if w not in stopWords:\n",
    "                text_filtered.append(w)\n",
    "        texts_filtered.append(text_filtered)\n",
    "        \n",
    "    return texts_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bummer', 'shoulda', 'david', 'carr', 'third']]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_filtered = removeSW(tweets_tokens)\n",
    "\n",
    "tweets_filtered[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['bummer', 'shoulda', 'david', 'carr', 'third']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['upset', 'update', 'facebook', 'texting', 'might', 'result', 'school', 'today', 'also']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['dived', 'many', 'times', 'ball', 'managed', 'save', 'rest', 'bounds']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['whole', 'body', 'feels', 'itchy', 'like', 'fire']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['behaving']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['whole', 'crew']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['need']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['long', 'time', 'rains', 'fine', 'thanks']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['nope']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['muera']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['spring', 'break', 'plain', 'city', 'snowing']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['pierced', 'ears']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['bear', 'watch', 'thought', 'loss', 'embarrassing']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['counts', 'either', 'never', 'talk', 'anymore']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['would', 'first', 'really', 'though', 'snyder', 'doucheclown']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['wish', 'watch', 'miss']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['hollis', 'death', 'scene', 'hurt', 'severely', 'watch', 'film', 'directors']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['file', 'taxes']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['always', 'wanted', 'rent', 'love']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['dear', 'drinking', 'forgotten', 'table', 'drinks']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['much', 'done']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['friend', 'called', 'asked', 'meet', 'valley', 'today', 'time', 'sigh']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['baked', 'cake', 'ated']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['week', 'going', 'hoped']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['blagh', 'class', 'tomorrow']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['hate', 'call', 'wake', 'people']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['going', 'sleep', 'watching', 'marley']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['miss', 'lilly']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['ooooh', 'leslie', 'leslie']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['almost', 'lover', 'exception', 'track', 'gets', 'depressed', 'every', 'time']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['some1', 'hacked', 'account', 'make']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['want', 'promote', 'gear', 'groove', 'unfornately', 'ride', 'going', 'anaheim', 'though']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['thought', 'sleeping', 'option', 'tomorrow', 'realizing', 'evaluations', 'morning', 'work']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['love', 'miss']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['asian', 'eyes', 'sleep', 'night']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['sick', 'spent', 'hour', 'sitting', 'shower', 'cause', 'sick', 'stand', 'held', 'back', 'puke', 'like', 'champ']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['tell', 'story', 'later', 'good', 'workin', 'like', 'three', 'hours']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['time', 'came']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['either', 'depressing', 'think', 'even', 'want', 'know', 'kids', 'suitcases']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['class', 'work', 'class', 'another', 'gonna', 'miss', 'girlfriend']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['really', 'feel', 'like', 'getting', 'today', 'study', 'tomorrows', 'practical', 'exam']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['reason', 'teardrops', 'guitar', 'enough', 'break', 'heart']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['know', 'hate', 'feeling', 'wanna', 'sleep', 'still']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['wish', 'finally', 'missed']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['falling', 'asleep', 'heard', 'tracy', 'girl', 'body', 'found', 'heart', 'breaks', 'family']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['happy', 'also', 'means', 'less', 'time']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['checked', 'user', 'timeline', 'blackberry', 'looks', 'like', 'twanking', 'still', 'happening', 'still', 'probs', 'uids']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['ironing', 'fave', 'wear', 'meeting', 'burnt']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['strangely', 'lilo', 'samro', 'breaking']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['sorry', 'think', 'retweeting']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['broadband', 'plan', 'massive', 'broken', 'promise', 'still', 'waiting', 'broadband']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['tons', 'replies', 'unfollow', 'friends', 'tweets', 'scrolling', 'feed']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['duck', 'chicken', 'taking', 'wayyy', 'long', 'hatch']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['vacation', 'photos', 'online', 'crashed', 'forget', 'name', 'site']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['need']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['sure', 'much', 'want', 'dont', 'think', 'trade', 'away', 'company', 'assets', 'sorry']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['hate', 'happens']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['feeling', 'dallas', 'going', 'show', 'gotta', 'though', 'think', 'shows', 'would', 'music', 'game']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['degrees', 'tomorrow']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['move', 'thought', 'already', 'hmmm', 'random', 'found', 'glad', 'hear', 'well']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['miss', 'commission', 'wutcha', 'playing', 'copped', 'blood', 'sand']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['leaving', 'parking']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['life', 'cool']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['sadly', 'though', 'never', 'gotten', 'experience', 'post', 'coitus', 'cigarette', 'never']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['nice', 'rain', 'comes', 'tomorrow']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['around', 'lost', 'even', 'phone', 'bill', 'lmao', 'shucks']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['damm', 'back', 'school', 'tomorrow']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['jobs', 'money', 'hell', 'wage', 'clams', 'hour']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['forever']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['agreed', 'failwhale', 'allllll', 'today']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['haha', 'dude', 'dont', 'really', 'look', 'unless', 'someone', 'says', 'added', 'sorry', 'terrible', 'need']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['sure', 'right', 'need', 'start', 'working', 'nikster', 'jared']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['really', 'hate', 'people', 'diss', 'trace', 'clearly']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['attire', 'today', 'puma', 'singlet', 'adidas', 'shorts', 'black', 'business', 'socks', 'leather', 'shoes', 'lucky', 'cute', 'girls']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['show']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['picnic', 'phone', 'smells', 'like', 'citrus']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['donkey', 'sensitive', 'comments', 'nevertheless', 'glad', 'asap', 'charger', 'still', 'awol']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['tonight']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['think', 'arms', 'sore', 'tennis']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['wonders', 'someone', 'like', 'much', 'make', 'unhappy', 'split', 'seccond', 'depressed']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['sleep', 'soon', 'hate', 'saying', 'tomorrow', 'night']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['newsletter', 'fares', 'really', 'unbelievable', 'shame', 'already', 'booked', 'paid', 'mine']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['missin']\n",
      "{'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0} []\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['damn', 'chalkboard', 'useless']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['blast', 'getty', 'villa', 'hates', 'sore', 'throat', 'getting', 'worse']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['missed', 'meeting', 'mama']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['tummy', 'hurts', 'wonder', 'hypnosis', 'anything', 'working', 'stop']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['always']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['sorry', 'annoys', 'thankfully', 'asleep', 'right', 'muahaha', 'evil', 'laugh']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['paid', 'attention', 'covered', 'photoshop', 'webpage', 'design', 'class', 'undergrad']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['wednesday', 'know']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['poor', 'cameron', 'hills']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['pray', 'please', 'threatening', 'start', 'babies', 'birthday', 'party', 'jerk', 'still', 'headache']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['really', 'enjoy', 'problems', 'constants', 'think', 'things', 'find', 'someone', 'ulike']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['strider', 'sick', 'little', 'puppy']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['rylee', 'grace', 'wana', 'steve', 'party', 'sadly', 'since', 'easter', 'able', 'much', 'well']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['actually', 'bracket', 'money']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['follow', 'either', 'work']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['nite', 'favorite', 'teams', 'astros', 'spartans', 'lose', 'nite', 'good']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['body', 'missing', 'northern', 'calif', 'girl', 'found', 'police', 'found', 'remains', 'missing', 'northern', 'california', 'girl']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['hope', 'increase', 'capacity', 'fast', 'yesterday', 'pain', 'fail', 'whale', 'times', 'hours']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['behind', 'classes', 'work']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['watching', 'quot', 'house', 'quot']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['remember', 'strikes', 'back', 'time', 'serious']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['cool', 'kinds', 'complaints', 'laptop', 'online', 'overheating', 'recalls']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['emily', 'glad', 'mommy', 'done', 'training', 'misses']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['would', 'rather', 'first', 'party', 'send', 'messages', 'party', 'send', 'mixed', 'ones', 'sophmore', 'year']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['overrated']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['heard', 'afternoon', 'wondered', 'thing', 'moscow', 'behind', 'times']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['laying', 'voice']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['sooo', 'killed', 'kutner', 'house', 'whyyyyyyyy']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['sorry', 'tell', 'culpa', 'really', 'sorry']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['make', 'sense', 'suicide', 'thing', 'refuse', 'believe', 'actually', 'happened']\n",
      "{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4404} ['hope']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['grind', 'inspirational', 'saddening', 'time', 'want', 'stop', 'like', 'much', 'love']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['yeah', 'know', 'wudnt', 'stand', 'chance']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['cant', 'sleep', '30am']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['hanging', 'crooners', 'wanna', 'sing', 'sucks']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['miss', 'leaving', 'tomorrow', 'quot', 'morning', 'quot', 'think', 'wanna', 'beach']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['pissed', 'asba', 'radio', 'station']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['wednesday', 'know']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['know', 'life', 'flipped', 'upside', 'thought', 'head', 'ramen', 'sounds', 'good']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['pain', 'back', 'sides', 'hurt', 'mention', 'crying', 'made', 'fail']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['late', 'night', 'snack', 'glass', 'quot', 'sickness', 'quot', 'back', 'sleep', 'hate', 'getting', 'sick']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['camilla', 'belle']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['clip', 'must', 'stupido', 'work', 'filters', 'wait', 'till', 'puter', 'something', 'else', 'blame', 'broke', 'mine']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['week', 'seems', 'longer', 'longer', 'terms', 'much', 'need', 'much', 'actually', 'going', 'done']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['cold']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['ehhh', 'weather', 'gonna', 'take', 'turn', 'ugly', 'tomorrow']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['haha', 'cooooold', 'still', 'show', 'incredible', 'stuff']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['hoping', 'tummy', 'rumbles', 'away', 'soon']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['notice', 'told', 'working', 'tomorrow', 'called', 'agency', 'follow', 'said']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['almost', 'bedtime']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['missing', 'babe', 'long', 'alive', 'happy', 'tired', 'love', 'imma', 'sleep', 'hopefully', 'headstart']\n",
      "{'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0} []\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['miss', 'kenny', 'powers']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['thank', 'letting', 'people', 'know', 'direct', 'message', 'actually', 'bridget']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['india', 'missed', '100th', 'test', 'victory', '10th', 'consecutive', 'without', 'loss']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['guess']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['sadly', 'going']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['shame', 'hear', 'stephan']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['leavin', 'morning']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['intending', 'finish', 'editing', 'page', 'novel', 'manuscript', 'tonight', 'probably', 'happen', 'pages', 'left']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['laid', 'around', 'much', 'today', 'head', 'hurts']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['still', 'read', '10th', 'princess', 'diaries', 'saving', 'francesca', 'made', 'easy', 'books']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['nokia', '1110', 'died']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['might', 'breast', 'cancer', 'find', 'anything', 'like', 'week']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['going', 'sleep', 'hoping', 'tomorrow', 'better']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['wish', 'understood', 'daylight', 'savings', 'ended', 'though', 'breakfast', 'hour', 'later', 'keep', 'waking', 'kids']\n",
      "{'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.4215} ['lame']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['understand', 'really']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['heroes', 'season']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['living', 'downtown', 'sure', 'much']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['calorie', 'wise', 'wish', 'junk', 'food', 'calorie', 'thing', 'sour', 'skittles', 'cherry']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['work', 'hard']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['getting', 'sick', 'time', 'studying', 'sleeeep']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['getting', 'eyebrows', 'waxed', 'pain']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['phantasy', 'star', 'yesterday', 'going', 'work']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['macheist', 'apps', 'sweet', 'espresso', 'serial', 'though', 'although', 'said', 'sent', 'well']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['picked', 'mich', 'feeling', 'pretty', 'good', 'pick', 'tonight', 'lost']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['alone', 'downstairs', 'working']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['feel']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['hates', 'anoop', 'mean', 'seriously', 'kinda', 'mean']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['sprint', 'baltimore', 'chicago']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['stuck', 'awake', 'middle', 'night', 'second', 'felt', 'terrible', 'yesterday']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['thanks', 'bursting', 'bubble']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['going', 'school', 'soon', 'find', 'anything', 'gosh', 'hard']\n",
      "{'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.0772} ['serious']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['page', 'sooooo', 'deleted', 'history']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['crazy', 'wind', 'today', 'birding']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['currently', 'work']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['grrr', 'ipods', 'acting', 'weird', 'thinking', 'playing', 'full', 'songs', 'ughh']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['send', 'missed', 'heaps', 'happy']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['deal', 'website']\n",
      "{'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.0772} ['sorry']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['rail', 'tips']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['still', 'swear', 'keep', 'losing', 'gaining', 'losing', 'gaining', 'tweeps', 'heart', 'wrenching']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['today', 'realized', 'good', 'hiding', 'things', 'even', 'find']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['staying', 'friends', 'house', 'house', 'sitting', 'neighbors', 'loud', 'party']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['danny', 'upset', 'wasnt', 'watch', 'live', 'chat', 'hours', 'trip', 'soooo']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['check']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['borders', 'closed']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['downloading', 'album', 'quot', 'slip', 'quot', 'hell', 'come', 'behind', 'times', 'days']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['woke', 'already', 'written', 'mail', 'early', 'university', 'today', 'teach']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['watching', 'hill', 'making']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['many', 'channels', 'boring', 'lazy', 'find', 'hobby']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['miss', 'buddy', '25th']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['love', 'french', 'tell', 'people', 'south', 'french', 'snarl', 'french', 'beautiful', 'people']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['opps', 'said', 'still', 'remain', 'problem', 'come']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['activated', 'selfcontrol', 'block', 'early', 'meaning', 'check', 'regularizing', 'internal', 'clock', 'might', 'difficult']\n",
      "{'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0} []\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['spencer', 'good']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['reese', 'dying', 'season', 'finale', 'next', 'week', 'boring', 'madame', 'president', 'crazy', 'woman']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['hate', 'limited', 'letters', 'hope', 'guys', 'fine', 'pray', 'well']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['shit', 'done', 'today', 'screwed']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['wanttss']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['going', 'sleep', 'tonite']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['worried', 'tired', 'post', 'tonight']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['shit', 'done', 'today', 'screwed']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['interview', 'cardiff', 'today', 'wish', 'hours', 'sleep']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['show', 'whack', 'worse', 'whack', 'wiggety', 'whack']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['really', 'think', 'people', 'choose', 'think', 'chose', 'accept', 'family', 'help', 'might', 'dead']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['going', 'kill', 'seen', 'waiting', 'till', 'solid', 'week', 'sitting']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['think']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['hate', 'spartans']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['mind', 'body', 'severely', 'protesting', 'quot', 'getting', 'quot', 'thing', 'nightmares', 'boot']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['goin', 'follow', 'since']\n",
      "{'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0} []\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['think', 'want', 'read', 'books', 'library']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['interrupted', 'many', 'times', 'today', 'going', 'japanese', 'rents']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['kind', 'longs', 'shows', 'ghost', 'world', 'right']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['canada', 'canada', 'weird', 'supposed', 'snow', 'wednesday']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['awwh', 'babs', 'look', 'underneith', 'shop', 'entrance', 'quot', 'yesterday', 'musik', 'quot', 'like', 'look', 'transformer', 'movie']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['feet', 'macbook', 'fell']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['gonna', 'late', 'tomorrow', '132am', 'gonna', 'tipsy', 'lonesome']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['sweating', 'forthcoming', 'trip', 'find', 'someone', 'crash', 'screwed']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['gotten', 'somebody', 'read', 'tweets', 'cant', 'make', 'account']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['omgawd', 'couldnt', 'handle', 'heat', 'time']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['hope', 'make', 'auburn', 'show', 'looking', 'good']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['thats', 'people', 'haha', 'couldnt', 'dont', 'think', 'pictures', 'ever', 'made', 'magazine', 'haha']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['totally', 'forgot', 'submit', 'photos']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['good', 'luck', 'please', 'work', 'hard', 'hope', 'album', 'gonna']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['leaving', 'gets']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['miss', 'twitter', 'phone', 'broke', 'using', 'stupid', 'nokia', 'phone', 'ughhh', 'miss', 'advance', 'phone']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['shooting', 'outside', 'house', 'scared']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['tuesday', 'start', 'reflection', 'lecture', 'stress', 'reducing', 'techniques', 'sure', 'might', 'become', 'useful', 'accompaniers']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['tragedy', 'disaster', 'news', 'week']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['still', 'trying', 'find', 'picture', 'upload', 'correclty']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['game', 'rained', 'looking', 'forward', 'opening']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['still', 'find', 'keys']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['know', 'right', 'dunno', 'going', 'twitter']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['might', 'getting', 'sore', 'throat']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['home', 'town', 'mammy', 'called', 'depressd', 'explain', 'parent', 'child', 'walk', 'alone', 'hello']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['think', 'need', 'find', 'better', 'anti', 'depressants', 'think', 'combo', 'losing', 'efficacy']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['restaurant', 'called', 'woodntap', 'competitive', 'eating', 'tourney', 'round', 'tourney', 'time', 'place']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['bathroom', 'wake', 'lakin']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['want', 'tacos', 'margarhitas', 'telll', 'hello']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['lonely', 'keep', 'female', 'california']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['betfair', 'office']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['miss', 'wait', 'celebrate', 'heel', 'weekend']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['really', 'cold', 'want', 'sleep', 'nothing']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['officially']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['monkeys', 'found', 'twin', 'wont', 'even', 'write', 'back', 'heartbroken']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['know', 'felt', 'like', 'yesterday', 'work']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['treaty', 'defined']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['missed', 'brent', 'praise', 'band', 'lead', 'guitarist', 'pout']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['poor', 'john', 'happens', 'play', 'fruit', 'seriously', 'though', 'seen', 'doctor']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['missing', 'watching', 'home', 'away', 'reminds', 'shout', 'courts']\n",
      "{'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0} []\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['video', 'card']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['feeling', 'lost', 'naked', 'confused', 'sort', 'iphone']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['damn', 'late', 'filling', 'appraisal', 'form', 'people', 'almost', 'sent', 'occupied', 'work']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['missed', 'brent', 'praise', 'band', 'lead', 'guitarist', 'pout']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['think', 'much', 'past', 'cant', 'change', 'deserved', 'much', 'still', 'thinking']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['lost', 'ring', 'seen']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['sealclap', 'download', 'shitloads', 'folders', '4chan', 'internet', 'moneys', 'fuck', 'yeah', 'alicia', 'mikey']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['still', 'nursing', 'nile', 'glad', 'feeling', 'better', 'hate', 'baby', 'sick']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['fucked', 'back']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['yoyoyo', 'internet', 'rude', 'tonight', 'reconnected']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['well', 'stuff', 'netball', 'netbal', 'done', 'stuff']\n",
      "{'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.4767} ['poor']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['help', 'forget', 'april', '13th']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['dierks', 'bentley', 'comin', 'columbus', 'wanna']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['take', 'sidekick', 'back']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['totally', 'wish', 'working']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['gr8t', 'face', 'itchy']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['poor', 'socks', 'luvvvvv', 'golden', 'want', 'sighhhh']\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} ['found', 'tracy', 'girl', 'piece', 'luggage', 'fucking', 'terrible']\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for comment in tweets_filtered:\n",
    "    print(analyzer.polarity_scores(comment),comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
